

# 知识蒸馏

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200910093559694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FuZHlqa3Q=,size_16,color_FFFFFF,t_70#pic_center)

## 一、实验思路：

1. 先将resnet34标准模型训练到最高的精度，然后将每隔2个epoch的模型保存下来。（teacher_model）

2. 再用resnet34标准模型最后一次训练出来的模型来指导小模型的训练。（小模型是取得resne34的前两层）

3. 再将大模型指导小模型进行训练，然后将小模型训练到最高的精度，然后将每隔2个epoch的模型保存下来。(student_model)

4. 再将大模型（teacher_model）和小模型（student_model）对应epoch的模型分别进行tracin值的计算，在分别对两个模型tracin

   值的分布进行比较，然后看两者是否接近。

## 二、知识蒸馏的思想：

### 1.提出：

​    虽然现在很多分类模型都采用交叉熵衡量预测值与真实值，然而真实值采用的one-hot向量所能提共的信息没有概率分布多。

​    原理：概率分布比one-hot更能提供信息-暗知识。

​    loss=0.7KL散度（softmax_t(老师输出））+0.3交叉熵（oneHot）

​    两个分布的loss可以用KL散度。

### 2.简单介绍：

知识蒸馏，可以将一个网络的知识转移到另一个网络，两个网络可以是同构或者异构。做法是先训练一个teacher网络，然后使用这个teacher网络的输出和数据的真实标签去训练student网络。知识蒸馏，可以用来将网络从大网络转化成一个小网络，并保留接近于大网络的性能；也可以将多个网络的学到的知识转移到一个网络中，使得单个网络的性能接近emsemble的结果。

<img src="F:\markdown文件\img\20210422115048747.png" style="zoom: 67%;" />

### 3.pytorch中的一些损失函数

<img src="F:\markdown文件\img\20210422115150214.png" style="zoom:67%;" />



### 4.核心

<img src="F:\markdown文件\img\20210422115223282.png" style="zoom:67%;" />

### 5.理论分析

<img src="F:\markdown文件\img\20210422115235117.png" style="zoom:67%;" />

### 6.关键代码

```python
def distillation(y,labels,teacher_scores,temp,alpha):
    return nn.KLDivLoss()(F.log_softmax(y/temp, dim=1),
                          F.softmax(teacher_scores/temp,dim=1))*(temp*temp*2.0*alpha)+F.cross_entropy(y,labels)*(1. - alpha)

```

![image-20220617180601852](C:\Users\凯凯凯\AppData\Roaming\Typora\typora-user-images\image-20220617180601852.png)

## 三、实验结果

|                            | 训练精度 | 验证精度 | 训练时间（1个epoch） | 测试时间（1个epoch） |
| :------------------------: | :------: | :------: | :------------------: | :------------------: |
|    resnet34（epoch=20）    |  0.803   |  0.877   |         514s         |         40s          |
| resnet-student（epoch=20） |  0.756   |  0.850   |         452s         |         26s          |

tracin值前多少个数据里面相同的值的数量（针对一个训练数据对所有的测试数据来说的）

| （teacher和student两个模型中）tracin值排名靠前的个数（epoch=20的checkpoint） | 100  | 500  | 800  | 1000 | 2000 |
| :----------------------------------------------------------: | :--: | :--: | :--: | :--: | ---- |
|                 靠前的个数里面相同的数据个数                 |  14  | 260  | 644  | 1000 | 1411 |
|         时间：teacher：28ms            student：12ms         |      |      |      |      |      |
|                                                              |      |      |      |      |      |







